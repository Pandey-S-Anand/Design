//Bruteforce approach

class MyHashSet {
    // The maximum possible key value in the HashSet (up to 10^6 as per the constraints)
    public static final int MAX_SIZE = 1000001;
    /*
     * Array to store the existence of keys in the HashSet.
     * The index represents the key, and the value at that index indicates whether
     * the key is present (true) or not (false).
     */
    private boolean[] set;

    public MyHashSet() {
        //Initialize the boolean array with size MAX_SIZE. All values are set to false initially, indicating no keys are present.
        set = new boolean[MAX_SIZE];
    }

    public void add(int key) {
        // Set the value at the index corresponding to the key to true,indicating the key is present in the set
        set[key] = true;
    }

    public void remove(int key) {
        // Set the value at the index corresponding to the key to false, indicating the key is no longer in the set.
        set[key] = false;
    }

    public boolean contains(int key) {
        // Return the value at the index corresponding to the key. If true, the key exists in the set, otherwise it does not.
        return set[key];
    }
}


/*
The size of the hash table is chosen based on prime number selection to reduce collisions in a hash table using separate chaining.

Why Choose a Prime Number for 'size'?

1. Reduces Collisions
   - A prime number as the array size spreads out the keys more uniformly.
   - This prevents patterns in the input keys from clustering into specific buckets.

2. Works Well with Modulo Hashing
   - The hash() function:
     private int hash(int key) {
         return key % size;
     }
   - If 'size' were a power of 2 (e.g., 1024), the modulo operation would only depend on 
     the least significant bits, leading to more collisions.
   - A prime number ensures that all bits contribute to the hash, leading to a better distribution.

3. Balances Space and Performance
   - If the array is too small, collisions increase, making operations slower (O(n) in worst case).
   - If the array is too large, space is wasted.

   How to Choose a Good Hash Table Size?
   - Estimate the number of elements (n) expected to be stored.
   - Choose a prime number slightly larger than n (e.g., n * 1.2).
   - If resizing dynamically, use prime numbers for rehashing (e.g., 1009 → 2003 → 4001 → 8009).
*/

/*
Given the constraints:

Key Range: 0 <= key <= 10^6  
Operations: At most 10^4 calls to add, remove, and contains

We need to choose a suitable hash table size ('size') to minimize collisions while keeping space usage efficient.  
The problem specifies at most 10^4 operations, but this does not guarantee 10^4 distinct elements.  
Assuming that many add operations will insert unique elements, we estimate n ≈ 10^4.

Final Choice for 'size': 10007  
- Large enough to handle up to 10^4 unique keys with minimal collisions.  
- Small enough to save space compared to using a table size of 10^6 (direct array).
*/


//Optimal approach

class MyHashSet {
    // Node class represents each element in the singly linked list used for separate chaining.
    private static class Node {
        int data; // Data stored in the node (the key).
        Node next; // Pointer to the next node in the chain.

        // Constructor to initialize the node with the given data.
        public Node(int data) {
            this.data = data;
            this.next = null;
        }
    }

    // Prime number chosen for the size of the hash table to reduce collisions.
    private static final int INTIAL_SIZE = 10007;
    private int currentSize; // Tracks the number of elements.
    /*
     * The load factor measures how full the hash table is and helps determine
     * when resizing is necessary to maintain efficient operations.
     *
     * Load Factor = (Number of Stored Elements) / (Total Number of Buckets)
     *
     * Example:
     * If there are 500 elements in a hash table with 1000 buckets:
     * 500 / 1000 = 0.5 → Load Factor = 0.5 (50%)
     *
     * - A **higher load factor** increases collisions, slowing down lookups.
     * - A **lower load factor** leads to wasted space due to many empty buckets.
     *
     * **Threshold for Resizing (Typically 0.75)**:
     * - Many implementations resize the hash table when the load factor exceeds **0.75**.
     * - This ensures that most buckets remain sparsely populated, keeping operations close to **O(1)**.
     */

    private static double loadFactor;

    // Array of buckets (each bucket holds a singly linked list of nodes with same hash value).
    private Node[] buckets;

    // Constructor to initialize the hash table with empty buckets.
    public MyHashSet() {
        buckets = new Node[INTIAL_SIZE];
        currentSize = 0;
        loadFactor = 0.75D;
    }

    public void add(int key) {
        // If the load factor exceeds 75%, resize the table.
        if ((double) currentSize / buckets.length >= loadFactor) {
            resize();
        }

        int index = hash(key); // Calculate the bucket index for the key.

        // If the bucket is empty, insert the new key.
        if (buckets[index] == null) {
            buckets[index] = new Node(key);
        } else {
            // Check if the key already exists in the singly linked list.
            Node currentNode = buckets[index];
            while (currentNode != null) {
                if (currentNode.data == key) {
                    return; // Key already exists, no need to add.
                }
                currentNode = currentNode.next;
            }
            // Add the new key at the front of the singly linked list if not found.
            Node newNode = new Node(key);
            newNode.next = buckets[index];
            buckets[index] = newNode;
        }
        currentSize++;
    }

    public boolean contains(int key) {
        int index = hash(key); // Calculate the bucket index for the key.
        Node currentNode = buckets[index]; // Start from the head of the singly linked list in the bucket.

        // Traverse the singly linked list in the bucket to search for the key.
        while (currentNode != null) {
            if (currentNode.data == key) {
                return true; // Key found.
            }
            currentNode = currentNode.next;
        }
        return false; // Key not found.
    }

    public void remove(int key) {
        int index = hash(key); // Calculate the bucket index for the key.
        Node previousNode = null;
        Node currentNode = buckets[index]; // Start from the head of the singly linked list in the bucket.

        // Traverse the singly linked list to find the key to remove.
        while (currentNode != null) {
            if (currentNode.data == key) {
                // If key is found, remove it from the singly linked list.
                if (currentNode == buckets[index]) {
                    // If the node to remove is the head, update the bucket to the next node.
                    buckets[index] = currentNode.next;
                } else {
                    // Otherwise, bypass the node by linking the previousNode node to the next node.
                    previousNode.next = currentNode.next;
                }
                currentSize--;
                return; // Key removed, exit.
            }
            previousNode = currentNode;
            currentNode = currentNode.next;
        }
    }

    // Hash function to calculate the index of the bucket for a given key.
    private int hash(int key) {
        return key % buckets.length; // Return the remainder of dividing the key by the size (number of buckets).
    }

    private void resize() {
        Node[] oldBuckets = buckets;
        buckets = new Node[oldBuckets.length * 2]; // Double the bucket size.
        currentSize = 0; // Reset size as add() will increment it

        // Rehash and insert all elements into the new hash table.
        for (Node head : oldBuckets) {
            while (head != null) {
                add(head.data);
                head = head.next;
            }
        }
    }
}
